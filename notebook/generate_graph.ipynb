{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99a07a0bd8374cce8aeee930685e4c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea954d238d8949168fd56a573071742d",
              "IPY_MODEL_d0f05236388c437b85c8e9834ac04643",
              "IPY_MODEL_c04aedce3e9642d98579ac028b32861a"
            ],
            "layout": "IPY_MODEL_cc65516a35d74aa0afd1798abfebd0ff"
          }
        },
        "ea954d238d8949168fd56a573071742d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fc00a0dc3c84dd0a00aeb6fe49bac0a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4d49bf0c67774371b45dde909c421d54",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "d0f05236388c437b85c8e9834ac04643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c341ae556bd4df4ad5cbf26c207ca17",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d317f3bd8bf04d10bbe1fb0cbb808f9e",
            "value": 2
          }
        },
        "c04aedce3e9642d98579ac028b32861a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_486401b511ed454a9b3feee93dc2e266",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d4eb87ab596848b39af0be876c98265a",
            "value": "‚Äá2/2‚Äá[00:02&lt;00:00,‚Äá‚Äá1.05it/s]"
          }
        },
        "cc65516a35d74aa0afd1798abfebd0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fc00a0dc3c84dd0a00aeb6fe49bac0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d49bf0c67774371b45dde909c421d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c341ae556bd4df4ad5cbf26c207ca17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d317f3bd8bf04d10bbe1fb0cbb808f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "486401b511ed454a9b3feee93dc2e266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4eb87ab596848b39af0be876c98265a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa69e05fe13745428ff83c3cc01941d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c78a4c1876164c73ac85ca8ca05ef79c",
              "IPY_MODEL_489d1a0572f3429b9089bea42839446e",
              "IPY_MODEL_49295fae6dc344dca69a52ac0c43ed84"
            ],
            "layout": "IPY_MODEL_67a44522060641c5a7db1817fa2f2012"
          }
        },
        "c78a4c1876164c73ac85ca8ca05ef79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42930ae0099f4582b1375d3789e49cb7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_47865e2ee4cb468aa5ade4e47089e8ce",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "489d1a0572f3429b9089bea42839446e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9885c62cd4d4827aed4717786d827f9",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ba8fcd9270b4110942b1723dcffe300",
            "value": 242
          }
        },
        "49295fae6dc344dca69a52ac0c43ed84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51dd78dae3924d96b4c76d84382c8fa1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0fc0b490deb14c9ea57df3068134d3a4",
            "value": "‚Äá242/242‚Äá[00:00&lt;00:00,‚Äá30.7kB/s]"
          }
        },
        "67a44522060641c5a7db1817fa2f2012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42930ae0099f4582b1375d3789e49cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47865e2ee4cb468aa5ade4e47089e8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9885c62cd4d4827aed4717786d827f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba8fcd9270b4110942b1723dcffe300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51dd78dae3924d96b4c76d84382c8fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc0b490deb14c9ea57df3068134d3a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "925dddd82a17402198a25c387049ea1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02a501fecad448ef87a106a7354422ba",
              "IPY_MODEL_4129eae49c05415ab546225092484a12",
              "IPY_MODEL_d2d880109ea847e4aa8f300a0b2f5aa9"
            ],
            "layout": "IPY_MODEL_02ea119fc2954b3592c28aa8f364a28a"
          }
        },
        "02a501fecad448ef87a106a7354422ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4c1f498be8470ea467d0d7e2ad3697",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2213a32c0fc649f8bfcf2a0daeb07cff",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4129eae49c05415ab546225092484a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d10248137241988543ea3fa616df0d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7531f50a63b248189c5ab3943d6ef7cd",
            "value": 2
          }
        },
        "d2d880109ea847e4aa8f300a0b2f5aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e13e1e841e8c46789730479a9279a65f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e50bd839051844ec80e856f212148012",
            "value": "‚Äá2/2‚Äá[00:01&lt;00:00,‚Äá‚Äá1.07it/s]"
          }
        },
        "02ea119fc2954b3592c28aa8f364a28a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4c1f498be8470ea467d0d7e2ad3697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2213a32c0fc649f8bfcf2a0daeb07cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2d10248137241988543ea3fa616df0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7531f50a63b248189c5ab3943d6ef7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e13e1e841e8c46789730479a9279a65f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50bd839051844ec80e856f212148012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sinh Knowledge Graph"
      ],
      "metadata": {
        "id": "voJ9dd6yBH6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes\n",
        "!pip install -q networkx tqdm pyyaml\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA3PYhyXkcOZ",
        "outputId": "f21e4318-48a5-45b5-cf9b-002c2710b455"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.9.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=253780426 sha256=4e2f9e39313266b1544b68138b15b91ee6221eccf14f7902b7c6620351340810\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import yaml\n",
        "import torch\n",
        "import gc\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from typing import List, Tuple, Union\n",
        "from tqdm import tqdm\n",
        "from networkx.readwrite import json_graph\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "f1E8ihOwkgvB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Model Configuration"
      ],
      "metadata": {
        "id": "JnmH35bNBLXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Model definition"
      ],
      "metadata": {
        "id": "41Z01XArlfnx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w9pUZQ7XBF_C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from typing import List\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "class LLM:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        use_quantization: bool = False # Tr√™n A100 v·ªõi model < 14B th√¨ ƒë·ªÉ False cho nhanh\n",
        "    ):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model_id = model_id\n",
        "\n",
        "        print(f\">> üì• Loading Model: {model_id}...\")\n",
        "        print(f\">> üöÄ Hardware Optimization: A100 Mode (Flash Attention 2 + bfloat16)\")\n",
        "\n",
        "        # C·∫•u h√¨nh load model\n",
        "        load_kwargs = {\n",
        "            \"device_map\": \"auto\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"attn_implementation\": \"flash_attention_2\", # TƒÉng t·ªëc c·ª±c m·∫°nh tr√™n A100\n",
        "            \"torch_dtype\": torch.bfloat16,              # Chu·∫©n t·ªët nh·∫•t cho A100\n",
        "        }\n",
        "\n",
        "        # Ch·ªâ b·∫≠t 4-bit n·∫øu model qu√° l·ªõn ho·∫∑c mu·ªën ti·∫øt ki·ªám VRAM\n",
        "        if use_quantization:\n",
        "            print(\">> ‚ö†Ô∏è Quantization Enabled (Slower but saves VRAM)\")\n",
        "            load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            # 1. Load Tokenizer TR∆Ø·ªöC (S·ª≠a l·ªói logic c≈©)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "            # 2. C·∫•u h√¨nh Padding (B·∫Øt bu·ªôc cho Batch Inference)\n",
        "            self.tokenizer.padding_side = \"left\"  # Decoder-only model ph·∫£i pad b√™n tr√°i\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # 3. Load Model\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                **load_kwargs\n",
        "            )\n",
        "            print(\">> ‚úÖ Model Loaded Successfully!\")\n",
        "\n",
        "            # Warmup nh·∫π\n",
        "            if self.device == \"cuda\":\n",
        "                print(f\">> üíæ VRAM Used: {torch.cuda.memory_allocated() / 1024**3:.2f} GiB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"‚ùå Failed to load model: {e}\")\n",
        "\n",
        "    def generate_response(self, input_text: str, system_prompt: str = \"You are a helpful AI assistant.\") -> str:\n",
        "        \"\"\"Sinh vƒÉn b·∫£n cho 1 input duy nh·∫•t (D√πng ƒë·ªÉ test)\"\"\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": input_text}\n",
        "        ]\n",
        "\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = self.model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.05\n",
        "            )\n",
        "\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):]\n",
        "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    def generate_batch(self, inputs: List[str], system_prompt: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Sinh vƒÉn b·∫£n cho nhi·ªÅu input c√πng l√∫c (T·ªëi ∆∞u cho A100)\n",
        "        \"\"\"\n",
        "        if not inputs: return []\n",
        "\n",
        "        # T·∫°o prompt cho c·∫£ batch\n",
        "        prompts = []\n",
        "        for text in inputs:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ]\n",
        "            prompts.append(\n",
        "                self.tokenizer.apply_chat_template(\n",
        "                    messages, tokenize=False, add_generation_prompt=True\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Tokenize & Padding\n",
        "        model_inputs = self.tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,       # Padding t·ª± ƒë·ªông\n",
        "            truncation=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = self.model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.15,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.05\n",
        "            )\n",
        "\n",
        "        # C·∫Øt b·ªè ph·∫ßn input (prompt) kh·ªèi output ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):]\n",
        "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        return self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    def unload_model(self):\n",
        "        print(f\">> üóëÔ∏è Unloading model {self.model_id}...\")\n",
        "        if hasattr(self, 'model'): del self.model\n",
        "        if hasattr(self, 'tokenizer'): del self.tokenizer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "        print(\">> ‚úÖ Memory cleared!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Model usage"
      ],
      "metadata": {
        "id": "IdpLHH9Ulh8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "def example():\n",
        "    print('=' * 60)\n",
        "    print('üöÄ A100 PERFORMANCE TEST: Qwen-2.5-3B (Bfloat16 + Flash Attention)')\n",
        "    print('=' * 60)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. LOAD MODEL\n",
        "    # ---------------------------------------------------------\n",
        "    print('\\n[1/4] Loading Model...')\n",
        "    start_load = time.time()\n",
        "\n",
        "    # Kh·ªüi t·∫°o class LLM (L∆∞u √Ω: ƒë·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y cell ƒë·ªãnh nghƒ©a class LLM ·ªü tr√™n)\n",
        "    generator = LLM(model_id=\"Qwen/Qwen2.5-3B-Instruct\", use_quantization=False)\n",
        "\n",
        "    end_load = time.time()\n",
        "    print(f'‚úÖ Model Loaded in: {end_load - start_load:.2f}s')\n",
        "    if torch.cuda.is_available():\n",
        "        print(f'üíæ VRAM Allocation: {torch.cuda.memory_allocated()/1024**3:.2f} GB')\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. SINGLE INFERENCE TEST (Ki·ªÉm tra logic)\n",
        "    # ---------------------------------------------------------\n",
        "    print('\\n[2/4] Testing Single Inference (Normal Chat)...')\n",
        "    print('-' * 40)\n",
        "\n",
        "    user_input = \"Gi·∫£i th√≠ch ng·∫Øn g·ªçn ƒë·ªãnh lu·∫≠t b·∫£o to√†n nƒÉng l∆∞·ª£ng.\"\n",
        "\n",
        "    start_single = time.time()\n",
        "    result = generator.generate_response(user_input)\n",
        "    end_single = time.time()\n",
        "\n",
        "    print(f\"‚ùì Input:  {user_input}\")\n",
        "    print(f\"üí° Output: {result}\")\n",
        "    print(f\"‚è±Ô∏è Time:   {end_single - start_single:.2f}s\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. BATCH INFERENCE TEST (Ki·ªÉm tra t·ªëc ƒë·ªô A100)\n",
        "    # ---------------------------------------------------------\n",
        "    print('\\n[3/4] Testing Batch Inference (High Throughput)...')\n",
        "    print(\"üëâ K·ªãch b·∫£n: X·ª≠ l√Ω song song 5 c√¢u h·ªèi c√πng l√∫c.\")\n",
        "    print('-' * 40)\n",
        "\n",
        "    batch_prompts = [\n",
        "        \"Th·ªß ƒë√¥ c·ªßa Ph√°p l√† g√¨?\",\n",
        "        \"Vi·∫øt c√¥ng th·ª©c t√≠nh di·ªán t√≠ch h√¨nh tr√≤n.\",\n",
        "        \"D·ªãch 'Hello world' sang ti·∫øng Vi·ªát.\",\n",
        "        \"Li·ªát k√™ 3 m√†u c∆° b·∫£n.\",\n",
        "        \"NƒÉm 2024 l√† nƒÉm con g√¨?\"\n",
        "    ]\n",
        "\n",
        "    start_batch = time.time()\n",
        "    # G·ªçi h√†m generate_batch m·ªõi th√™m v√†o\n",
        "    batch_results = generator.generate_batch(batch_prompts, system_prompt=\"B·∫°n l√† tr·ª£ l√Ω ng·∫Øn g·ªçn.\")\n",
        "    end_batch = time.time()\n",
        "\n",
        "    total_time = end_batch - start_batch\n",
        "\n",
        "    for i, (inp, out) in enumerate(zip(batch_prompts, batch_results)):\n",
        "        print(f\"  [{i+1}] Q: {inp} | A: {out.strip()}\")\n",
        "\n",
        "    print('-' * 40)\n",
        "    print(f\"‚è±Ô∏è Total Batch Time: {total_time:.2f}s (cho {len(batch_prompts)} c√¢u)\")\n",
        "    print(f\"‚ö° Average Speed:    {total_time / len(batch_prompts):.2f}s/c√¢u\")\n",
        "    print(\">> Nh·∫≠n x√©t: Batch size c√†ng l·ªõn, th·ªùi gian trung b√¨nh/c√¢u s·∫Ω c√†ng gi·∫£m tr√™n A100.\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. CLEANUP\n",
        "    # ---------------------------------------------------------\n",
        "    print('\\n[4/4] Cleanup...')\n",
        "    generator.unload_model()\n",
        "    print('=' * 60)\n",
        "    print('DONE.')"
      ],
      "metadata": {
        "id": "Bh_Hv8k2JM51"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726,
          "referenced_widgets": [
            "99a07a0bd8374cce8aeee930685e4c64",
            "ea954d238d8949168fd56a573071742d",
            "d0f05236388c437b85c8e9834ac04643",
            "c04aedce3e9642d98579ac028b32861a",
            "cc65516a35d74aa0afd1798abfebd0ff",
            "1fc00a0dc3c84dd0a00aeb6fe49bac0a",
            "4d49bf0c67774371b45dde909c421d54",
            "8c341ae556bd4df4ad5cbf26c207ca17",
            "d317f3bd8bf04d10bbe1fb0cbb808f9e",
            "486401b511ed454a9b3feee93dc2e266",
            "d4eb87ab596848b39af0be876c98265a",
            "fa69e05fe13745428ff83c3cc01941d0",
            "c78a4c1876164c73ac85ca8ca05ef79c",
            "489d1a0572f3429b9089bea42839446e",
            "49295fae6dc344dca69a52ac0c43ed84",
            "67a44522060641c5a7db1817fa2f2012",
            "42930ae0099f4582b1375d3789e49cb7",
            "47865e2ee4cb468aa5ade4e47089e8ce",
            "a9885c62cd4d4827aed4717786d827f9",
            "5ba8fcd9270b4110942b1723dcffe300",
            "51dd78dae3924d96b4c76d84382c8fa1",
            "0fc0b490deb14c9ea57df3068134d3a4"
          ]
        },
        "id": "oKWYyEnvJPQi",
        "outputId": "a13c896c-9e06-40c1-df51-7a86d5af32c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üöÄ A100 PERFORMANCE TEST: Qwen-2.5-3B (Bfloat16 + Flash Attention)\n",
            "============================================================\n",
            "\n",
            "[1/4] Loading Model...\n",
            ">> üì• Loading Model: Qwen/Qwen2.5-3B-Instruct...\n",
            ">> üöÄ Hardware Optimization: A100 Mode (Flash Attention 2 + bfloat16)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99a07a0bd8374cce8aeee930685e4c64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa69e05fe13745428ff83c3cc01941d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> ‚úÖ Model Loaded Successfully!\n",
            ">> üíæ VRAM Used: 5.75 GiB\n",
            "‚úÖ Model Loaded in: 4.51s\n",
            "üíæ VRAM Allocation: 5.75 GB\n",
            "\n",
            "[2/4] Testing Single Inference (Normal Chat)...\n",
            "----------------------------------------\n",
            "‚ùì Input:  Gi·∫£i th√≠ch ng·∫Øn g·ªçn ƒë·ªãnh lu·∫≠t b·∫£o to√†n nƒÉng l∆∞·ª£ng.\n",
            "üí° Output: ƒê·ªãnh lu·∫≠t b·∫£o to√†n nƒÉng l∆∞·ª£ng l√† m·ªôt nguy√™n t·∫Øc c∆° b·∫£n trong v·∫≠t l√Ω, cho bi·∫øt r·∫±ng t·ªïng l∆∞·ª£ng nƒÉng l∆∞·ª£ng trong m·ªôt h·ªá th·ªëng kh√¥ng thay ƒë·ªïi theo th·ªùi gian, ch·ªâ chuy·ªÉn t·ª´ m·ªôt d·∫°ng th√†nh d·∫°ng kh√°c. Nghƒ©a l√†, nƒÉng l∆∞·ª£ng kh√¥ng bao gi·ªù ƒë∆∞·ª£c t·∫°o ra hay m·∫•t ƒëi m√† ch·ªâ chuy·ªÉn ƒë·ªïi t·ª´ m·ªôt d·∫°ng sang d·∫°ng kh√°c. ƒê√¢y l√† m·ªôt trong nh·ªØng nguy√™n t·∫Øc c∆° b·∫£n nh·∫•t trong v·∫≠t l√Ω v√† c√≥ ·ª©ng d·ª•ng r·ªông r√£i trong nhi·ªÅu lƒ©nh v·ª±c khoa h·ªçc v√† c√¥ng ngh·ªá.\n",
            "‚è±Ô∏è Time:   7.91s\n",
            "\n",
            "[3/4] Testing Batch Inference (High Throughput)...\n",
            "üëâ K·ªãch b·∫£n: X·ª≠ l√Ω song song 5 c√¢u h·ªèi c√πng l√∫c.\n",
            "----------------------------------------\n",
            "  [1] Q: Th·ªß ƒë√¥ c·ªßa Ph√°p l√† g√¨? | A: Th·ªß ƒë√¥ c·ªßa Ph√°p l√† Paris.\n",
            "  [2] Q: Vi·∫øt c√¥ng th·ª©c t√≠nh di·ªán t√≠ch h√¨nh tr√≤n. | A: Di·ªán t√≠ch h√¨nh tr√≤n l√† œÄr¬≤, trong ƒë√≥ r l√† b√°n k√≠nh c·ªßa h√¨nh tr√≤n.\n",
            "  [3] Q: D·ªãch 'Hello world' sang ti·∫øng Vi·ªát. | A: 'Helo world' trong ti·∫øng Vi·ªát l√† 'Xin ch√†o th·∫ø gi·ªõi'.\n",
            "  [4] Q: Li·ªát k√™ 3 m√†u c∆° b·∫£n. | A: 3 m√†u c∆° b·∫£n l√† tr·∫Øng, ƒëen v√† ƒë·ªè.\n",
            "  [5] Q: NƒÉm 2024 l√† nƒÉm con g√¨? | A: NƒÉm 2024 l√† nƒÉm B√≠nh D·∫ßn.\n",
            "----------------------------------------\n",
            "‚è±Ô∏è Total Batch Time: 1.74s (cho 5 c√¢u)\n",
            "‚ö° Average Speed:    0.35s/c√¢u\n",
            ">> Nh·∫≠n x√©t: Batch size c√†ng l·ªõn, th·ªùi gian trung b√¨nh/c√¢u s·∫Ω c√†ng gi·∫£m tr√™n A100.\n",
            "\n",
            "[4/4] Cleanup...\n",
            ">> üóëÔ∏è Unloading model Qwen/Qwen2.5-3B-Instruct...\n",
            ">> ‚úÖ Memory cleared!\n",
            "============================================================\n",
            "DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Graph Generator"
      ],
      "metadata": {
        "id": "DXQCX3zdnS9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Generator Setup"
      ],
      "metadata": {
        "id": "KSeziG-9nknW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_configs(path):\n",
        "    print(f\">> Reading config from: {path}\")\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        return config\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file config t·∫°i: {path}. D√πng default.\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói YAML: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "iZHRPqc2npi8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConstructor:\n",
        "    E2E_PROMPT = (\n",
        "        \"B·∫°n l√† m·ªôt chuy√™n gia v·ªÅ X√¢y d·ª±ng ƒê·ªì th·ªã Tri th·ª©c (Knowledge Graph) Ti·∫øng Vi·ªát.\\n\"\n",
        "        \"Nhi·ªám v·ª•: Tr√≠ch xu·∫•t c√°c b·ªô ba ng·ªØ nghƒ©a (Ch·ªß th·ªÉ, Quan h·ªá, ƒê·ªëi t∆∞·ª£ng) t·ª´ vƒÉn b·∫£n ƒë∆∞·ª£c cung c·∫•p.\\n\\n\"\n",
        "        \"### QUY T·∫ÆC B·∫ÆT BU·ªòC:\\n\"\n",
        "        \"1. **NG√îN NG·ªÆ**: To√†n b·ªô k·∫øt qu·∫£ (relation, type, description...) PH·∫¢I L√Ä TI·∫æNG VI·ªÜT. TUY·ªÜT ƒê·ªêI KH√îNG D·ªäCH sang ti·∫øng Anh.\\n\"\n",
        "        \"2. **Quan h·ªá (Relation)**: Ph·∫£i tr√≠ch xu·∫•t c·ª•m ƒë·ªông t·ª´ ho·∫∑c gi·ªõi t·ª´ xu·∫•t hi·ªán trong vƒÉn b·∫£n g·ªëc (V√≠ d·ª•: 'l√† th·ªß ƒë√¥ c·ªßa', 't·ªça l·∫°c t·∫°i', 'ƒë·∫ßu t∆∞ v√†o').\\n\"\n",
        "        \"3. **Th·ª±c th·ªÉ (Head/Tail)**: Gi·ªØ nguy√™n t√™n ri√™ng v√† danh t·ª´ g·ªëc.\\n\"\n",
        "        \"4. **ƒê·ªìng tham chi·∫øu**: Thay th·∫ø c√°c ƒë·∫°i t·ª´ ('n√≥', '√¥ng ·∫•y') b·∫±ng t√™n th·ª±c th·ªÉ c·ª• th·ªÉ n·∫øu bi·∫øt.\\n\\n\"\n",
        "        \"5. **S·ª¨A L·ªñI INPUT N·∫æU C·∫¶N**: M·ªôt s·ªë Input c√≥ th·ªÉ s·∫Ω sai ch√≠nh t·∫£, h√£y s·ª≠a n·∫øu c·∫ßn thi·∫øt. L∆ØU √ù: CH·ªà THAY ƒê·ªîI WORD-BY-WORD.\"\n",
        "        \"### ƒê·ªäNH D·∫†NG OUTPUT (JSON ONLY):\\n\"\n",
        "        \"Ch·ªâ tr·∫£ v·ªÅ JSON object theo c·∫•u tr√∫c sau, kh√¥ng th√™m l·ªùi d·∫´n:\\n\"\n",
        "        \"{\\n\"\n",
        "        \"  \\\"graph\\\": [\\n\"\n",
        "        \"    {\\n\"\n",
        "        \"      \\\"head\\\": \\\"T√™n th·ª±c th·ªÉ 1\\\",\\n\"\n",
        "        \"      \\\"head_props\\\": {\\\"type\\\": \\\"Lo·∫°i th·ª±c th·ªÉ (VD: Qu·ªëc gia, Ng∆∞·ªùi)\\\", \\\"description\\\": \\\"M√¥ t·∫£ ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát\\\"},\\n\"\n",
        "        \"      \\\"relation\\\": \\\"ƒë·ªông t·ª´ quan h·ªá (ti·∫øng Vi·ªát)\\\",\\n\"\n",
        "        \"      \\\"relation_props\\\": {\\\"time\\\": \\\"th·ªùi gian (n·∫øu c√≥)\\\", \\\"loc\\\": \\\"ƒë·ªãa ƒëi·ªÉm (n·∫øu c√≥)\\\"},\\n\"\n",
        "        \"      \\\"tail\\\": \\\"T√™n th·ª±c th·ªÉ 2\\\",\\n\"\n",
        "        \"      \\\"tail_props\\\": {\\\"type\\\": \\\"Lo·∫°i th·ª±c th·ªÉ\\\", \\\"description\\\": \\\"M√¥ t·∫£\\\"}\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"  ]\\n\"\n",
        "        \"}\"\n",
        "    )\n",
        "\n",
        "    def __init__(self, config_path: str = None, llm_instance = None):\n",
        "        \"\"\"\n",
        "        Modified: Th√™m tham s·ªë llm_instance ƒë·ªÉ truy·ªÅn model ƒë√£ load b√™n ngo√†i v√†o (tr√°nh load l·∫°i).\n",
        "        N·∫øu kh√¥ng truy·ªÅn, gi·ªØ logic c≈©: t·ª± load config v√† t·ª± kh·ªüi t·∫°o LLM.\n",
        "        \"\"\"\n",
        "        if llm_instance:\n",
        "            self.llm = llm_instance\n",
        "        else:\n",
        "            # Logic c≈©: T·ª± load n·∫øu kh√¥ng truy·ªÅn instance v√†o\n",
        "            # (Gi·∫£ l·∫≠p load_configs ƒë·ªÉ code ch·∫°y ƒë∆∞·ª£c ƒë·ªôc l·∫≠p trong v√≠ d·ª• n√†y)\n",
        "            # full_config = load_configs(config_path)\n",
        "            # self.config = full_config.get('GraphConstructor', {})\n",
        "\n",
        "            # Hardcode fallback n·∫øu kh√¥ng c√≥ file config th·ª±c t·∫ø\n",
        "            self.re_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "            self.llm = LLM(model_id=self.re_model_name)\n",
        "\n",
        "        self.graph = nx.DiGraph()\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        return text.strip().replace('\"', \"'\")\n",
        "\n",
        "    def _parse_json_response(self, text: str) -> List[dict]:\n",
        "        # [LOGIC C≈® GI·ªÆ NGUY√äN]\n",
        "        try:\n",
        "            data = json.loads(text)\n",
        "        except json.JSONDecodeError:\n",
        "            try:\n",
        "                match = re.search(r\"```(?:json)?(.*?)```\", text, re.DOTALL)\n",
        "                if match:\n",
        "                    json_str = match.group(1).strip()\n",
        "                else:\n",
        "                    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "                    if match:\n",
        "                        json_str = match.group(0).strip()\n",
        "                    else:\n",
        "                        return []\n",
        "                data = json.loads(json_str)\n",
        "            except:\n",
        "                return []\n",
        "\n",
        "        if \"graph\" in data and isinstance(data[\"graph\"], list):\n",
        "            return data[\"graph\"]\n",
        "        if isinstance(data, list):\n",
        "            return data\n",
        "        return []\n",
        "\n",
        "    # [MODIFIED] Thay th·∫ø _extract_e2e ƒë∆°n l·∫ª b·∫±ng x·ª≠ l√Ω danh s√°ch inputs\n",
        "    def _extract_batch_e2e(self, texts: List[str]) -> List[List[dict]]:\n",
        "        # Chu·∫©n b·ªã inputs ƒë√∫ng format c≈©\n",
        "        user_inputs = [f\"Analyze this Vietnamese text and output JSON only:\\n\\\"{text}\\\"\" for text in texts]\n",
        "\n",
        "        try:\n",
        "            # G·ªçi h√†m generate_batch c·ªßa class LLM\n",
        "            response_contents = self.llm.generate_batch(\n",
        "                inputs=user_inputs,\n",
        "                system_prompt=self.E2E_PROMPT\n",
        "            )\n",
        "\n",
        "            # Parse t·ª´ng k·∫øt qu·∫£ tr·∫£ v·ªÅ\n",
        "            parsed_results = []\n",
        "            for content in response_contents:\n",
        "                parsed_results.append(self._parse_json_response(content))\n",
        "            return parsed_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå [Batch Extraction Error] {e}\")\n",
        "            # Tr·∫£ v·ªÅ list r·ªóng t∆∞∆°ng ·ª©ng v·ªõi s·ªë l∆∞·ª£ng input n·∫øu l·ªói\n",
        "            return [[] for _ in texts]\n",
        "\n",
        "    def _build_graph_from_triples(self, relations: List[dict]) -> nx.DiGraph:\n",
        "        # [LOGIC C≈® GI·ªÆ NGUY√äN]\n",
        "        G = nx.DiGraph()\n",
        "        if not relations: return G\n",
        "        for item in relations:\n",
        "            head = item.get(\"head\")\n",
        "            tail = item.get(\"tail\")\n",
        "            rel_label = item.get(\"relation\")\n",
        "            if not head or not tail or not rel_label: continue\n",
        "\n",
        "            head = str(head).strip()\n",
        "            tail = str(tail).strip()\n",
        "\n",
        "            G.add_node(head, **item.get(\"head_props\", {}))\n",
        "            G.add_node(tail, **item.get(\"tail_props\", {}))\n",
        "\n",
        "            edge_attrs = {\"relation\": rel_label}\n",
        "            edge_attrs.update(item.get(\"relation_props\", {}))\n",
        "            G.add_edge(head, tail, **edge_attrs)\n",
        "        return G\n",
        "\n",
        "    def process(self, data_input, text_column=None, verbose=True, batch_size=256):\n",
        "        \"\"\"\n",
        "        [MODIFIED] S·ª≠a logic loop ƒë·ªÉ ch·∫°y theo batch_size\n",
        "        \"\"\"\n",
        "        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o (Gi·ªØ logic c≈©)\n",
        "        if isinstance(data_input, pd.DataFrame):\n",
        "            data = data_input[text_column].tolist()\n",
        "        elif isinstance(data_input, pd.Series):\n",
        "            data = data_input.tolist()\n",
        "        elif isinstance(data_input, list):\n",
        "            data = data_input\n",
        "        else:\n",
        "            data = []\n",
        "\n",
        "        list_of_graphs = []\n",
        "        extraction_history = []\n",
        "\n",
        "        # X·ª≠ l√Ω text r·ªóng ho·∫∑c None tr∆∞·ªõc ƒë·ªÉ tr√°nh l·ªói model\n",
        "        processed_data = [str(x) if x else \"\" for x in data]\n",
        "        total_samples = len(processed_data)\n",
        "\n",
        "        # 2. V√≤ng l·∫∑p Batch (Thay cho v√≤ng l·∫∑p ƒë∆°n)\n",
        "        desc_text = f\"Building Graphs (Batch Size={batch_size})\"\n",
        "\n",
        "        for i in tqdm(range(0, total_samples, batch_size), desc=desc_text, disable=not verbose):\n",
        "            # C·∫Øt batch\n",
        "            batch_texts = processed_data[i : i + batch_size]\n",
        "\n",
        "            # G·ªçi h√†m x·ª≠ l√Ω batch m·ªõi\n",
        "            batch_triples = self._extract_batch_e2e(batch_texts)\n",
        "\n",
        "            # L∆∞u k·∫øt qu·∫£ l·∫°i (Mapping 1-1 v·ªõi input)\n",
        "            for text, triples in zip(batch_texts, batch_triples):\n",
        "                # X√¢y ƒë·ªì th·ªã cho t·ª´ng m·∫´u (Logic c≈©)\n",
        "                local_graph = self._build_graph_from_triples(triples)\n",
        "                list_of_graphs.append(local_graph)\n",
        "                extraction_history.append({\"input\": text, \"triples\": triples})\n",
        "\n",
        "        return list_of_graphs, extraction_history"
      ],
      "metadata": {
        "id": "fKP7AEjfoGxt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pipeline Setup"
      ],
      "metadata": {
        "id": "qpv9E__YrmLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Dataset loading"
      ],
      "metadata": {
        "id": "1LDCLFP6tw_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    print('> Start loading dataset...')\n",
        "\n",
        "    file_ids = {\n",
        "        'train': '1s7kEVugop7Gv_iws5Kv7OyvLnoRVzjCA',\n",
        "        'dev':   '1IrCff4cReAbdonve5__9fR3FC105d-LP',\n",
        "        'test':  '1NPWG2KVeKQ6FPck851wCobPxDJIU3l5_'\n",
        "    }\n",
        "\n",
        "    base_url = 'https://drive.google.com/uc?id='\n",
        "    dfs = {}\n",
        "    try:\n",
        "        for name, file_id in file_ids.items():\n",
        "            dl_url = f\"{base_url}{file_id}\"\n",
        "            print(f'  - Downloading & Reading {name} data...')\n",
        "            dfs[name] = pd.read_csv(dl_url)\n",
        "\n",
        "        print('> ‚úÖ Dataset loaded successfully.')\n",
        "        return dfs['train'], dfs['dev'], dfs['test']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {e}\")\n",
        "        print(\"M·∫πo: H√£y ch·∫Øc ch·∫Øn r·∫±ng link ƒë√£ b·∫≠t ch·∫ø ƒë·ªô 'Anyone with the link' (Public).\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "cWsnM_2ttC5I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, dev, test = load_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0lV5HPXt0BU",
        "outputId": "204c692e-3770-4119-bd9b-f0fff9db0191"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Start loading dataset...\n",
            "  - Downloading & Reading train data...\n",
            "  - Downloading & Reading dev data...\n",
            "  - Downloading & Reading test data...\n",
            "> ‚úÖ Dataset loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Main pipeline"
      ],
      "metadata": {
        "id": "ZvMshStduHeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from networkx.readwrite import json_graph\n",
        "\n",
        "def main():\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. SETUP: Kh·ªüi t·∫°o Model & Graph Engine (Ch·ªâ 1 l·∫ßn)\n",
        "    # ---------------------------------------------------------\n",
        "    # Kh·ªüi t·∫°o LLM A100 Mode (Native Bfloat16 + Flash Attention)\n",
        "    print(\">> üöÄ Initializing LLM Engine for A100...\")\n",
        "    llm_engine = LLM(model_id=\"Qwen/Qwen2.5-3B-Instruct\")\n",
        "\n",
        "    # T·∫°o config file n·∫øu ch∆∞a c√≥ (Logic c≈©)\n",
        "    if not os.path.exists('configs/graph_constructor.yml'):\n",
        "        os.makedirs('configs', exist_ok=True)\n",
        "        with open('configs/graph_constructor.yml', 'w') as f:\n",
        "            f.write(\"GraphConstructor:\\n  RE_MODEL_NAME: 'Qwen/Qwen2.5-3B-Instruct'\")\n",
        "\n",
        "    # Truy·ªÅn LLM engine v√†o ƒë·ªÉ kh√¥ng ph·∫£i load l·∫°i\n",
        "    kg_processor = GraphConstructor(\n",
        "        config_path='configs/graph_constructor.yml',\n",
        "        llm_instance=llm_engine\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. LOAD DATA\n",
        "    # ---------------------------------------------------------\n",
        "    train, dev, test = load_dataset()\n",
        "\n",
        "    if train is None:\n",
        "        print(\"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ch·∫°y.\")\n",
        "        return\n",
        "\n",
        "    datasets = [\n",
        "        # (\"dev\", dev),\n",
        "        # (\"test\", test),\n",
        "        (\"train\", train),\n",
        "    ]\n",
        "\n",
        "    # C·∫•u h√¨nh Batch Size cho A100 (32-64 l√† l√Ω t∆∞·ªüng cho model 3B)\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. PROCESSING LOOP\n",
        "    # ---------------------------------------------------------\n",
        "    for name, data in datasets:\n",
        "        print('=' * 50)\n",
        "        print(f\"Processing dataset: [{name}]\")\n",
        "        print(f'Shape: {data.shape}')\n",
        "        print('=' * 50)\n",
        "\n",
        "        # --- DEBUG MODE (Gi·ªØ logic c≈©: Ch·∫°y th·ª≠ v√†i d√≤ng) ---\n",
        "        # B·ªè comment d√≤ng d∆∞·ªõi ƒë·ªÉ test nhanh (Ch·ªâ ch·∫°y 1 batch ƒë·∫ßu ti√™n)\n",
        "        # data = data.head(5)\n",
        "\n",
        "        result = {\n",
        "            'id': [],\n",
        "            'context': [],\n",
        "            'prompt': [],\n",
        "            'response': [],\n",
        "            'label': [],\n",
        "        }\n",
        "\n",
        "        # Chia dataframe th√†nh c√°c chunk ƒë·ªÉ x·ª≠ l√Ω batch m√† kh√¥ng tr√†n RAM\n",
        "        # Thay v√¨ iterrows, ta loop theo index nh·∫£y c√≥c (0, 32, 64...)\n",
        "        total_rows = len(data)\n",
        "\n",
        "        for start_idx in tqdm(range(0, total_rows, BATCH_SIZE), desc=f\"Batch Run {name}\"):\n",
        "            # L·∫•y 1 chunk d·ªØ li·ªáu (v√≠ d·ª• 32 d√≤ng)\n",
        "            end_idx = min(start_idx + BATCH_SIZE, total_rows)\n",
        "            batch_df = data.iloc[start_idx : end_idx]\n",
        "\n",
        "            # Chu·∫©n b·ªã list text\n",
        "            # L∆∞u √Ω: fillna(\"\") ƒë·ªÉ tr√°nh l·ªói model ch·∫øt khi g·∫∑p NaN\n",
        "            contexts = batch_df['context'].fillna(\"\").tolist()\n",
        "            prompts  = batch_df['prompt'].fillna(\"\").tolist()\n",
        "            responses = batch_df['response'].fillna(\"\").tolist()\n",
        "\n",
        "            # --- Extract KG (Batch Inference) ---\n",
        "            # G·ªçi h√†m process c·ªßa kg_processor (ƒë√£ s·ª≠a ·ªü b∆∞·ªõc tr∆∞·ªõc)\n",
        "            # H√†m n√†y gi·ªù nh·∫≠n v√†o LIST v√† tr·∫£ v·ªÅ LIST Graphs\n",
        "\n",
        "            # 1. Batch Context\n",
        "            g_contexts, _ = kg_processor.process(contexts, batch_size=BATCH_SIZE, verbose=False)\n",
        "\n",
        "            # 2. Batch Prompt\n",
        "            g_prompts, _ = kg_processor.process(prompts, batch_size=BATCH_SIZE, verbose=False)\n",
        "\n",
        "            # 3. Batch Response\n",
        "            g_responses, _ = kg_processor.process(responses, batch_size=BATCH_SIZE, verbose=False)\n",
        "\n",
        "            # --- Post-process & Store ---\n",
        "            # Gh√©p k·∫øt qu·∫£ l·∫°i v√†o result dict (Logic c≈©: node_link_data)\n",
        "            ids = batch_df.get('id', range(start_idx, end_idx)).tolist()\n",
        "            labels = batch_df.get('label', [0]*len(batch_df)).tolist()\n",
        "\n",
        "            for i in range(len(batch_df)):\n",
        "                result['id'].append(ids[i])\n",
        "                result['label'].append(labels[i])\n",
        "\n",
        "                # Convert NetworkX -> JSON Dict (Logic c≈©)\n",
        "                result['context'].append(json_graph.node_link_data(g_contexts[i]))\n",
        "                result['prompt'].append(json_graph.node_link_data(g_prompts[i]))\n",
        "                result['response'].append(json_graph.node_link_data(g_responses[i]))\n",
        "\n",
        "            #if start_idx > 4: break\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 4. SAVE RESULTS\n",
        "        # ---------------------------------------------------------\n",
        "        output_dir = 'data/processed'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        output_path = f'{output_dir}/graph_{name}.csv'\n",
        "        print(f'\\n>> Saving {name} graph data to {output_path}...')\n",
        "\n",
        "        result_df = pd.DataFrame(result)\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "        print(\"Done.\")\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 5. CLEANUP & DOWNLOAD\n",
        "    # ---------------------------------------------------------\n",
        "    # Gi·∫£i ph√≥ng VRAM\n",
        "    if hasattr(kg_processor, 'llm'):\n",
        "        kg_processor.llm.unload_model()\n",
        "\n",
        "    print(\"\\n>> üì¶ Zipping and downloading results...\")\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        os.system(\"zip -r processed_data.zip data/processed\")\n",
        "        files.download('processed_data.zip')\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Kh√¥ng ph·∫£i m√¥i tr∆∞·ªùng Colab, file ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i folder data/processed.\")\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "DdNsLBxYrown"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "925dddd82a17402198a25c387049ea1c",
            "02a501fecad448ef87a106a7354422ba",
            "4129eae49c05415ab546225092484a12",
            "d2d880109ea847e4aa8f300a0b2f5aa9",
            "02ea119fc2954b3592c28aa8f364a28a",
            "8d4c1f498be8470ea467d0d7e2ad3697",
            "2213a32c0fc649f8bfcf2a0daeb07cff",
            "e2d10248137241988543ea3fa616df0d",
            "7531f50a63b248189c5ab3943d6ef7cd",
            "e13e1e841e8c46789730479a9279a65f",
            "e50bd839051844ec80e856f212148012"
          ]
        },
        "id": "bdMduiYNuRcp",
        "outputId": "06f642a3-b47f-45f7-fd41-452e47e9faca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> üöÄ Initializing LLM Engine for A100...\n",
            ">> üì• Loading Model: Qwen/Qwen2.5-3B-Instruct...\n",
            ">> üöÄ Hardware Optimization: A100 Mode (Flash Attention 2 + bfloat16)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "925dddd82a17402198a25c387049ea1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> ‚úÖ Model Loaded Successfully!\n",
            ">> üíæ VRAM Used: 13.89 GiB\n",
            "> Start loading dataset...\n",
            "  - Downloading & Reading train data...\n",
            "  - Downloading & Reading dev data...\n",
            "  - Downloading & Reading test data...\n",
            "> ‚úÖ Dataset loaded successfully.\n",
            "==================================================\n",
            "Processing dataset: [train]\n",
            "Shape: (5600, 5)\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batch Run train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [2:29:59<00:00, 204.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">> Saving train graph data to data/processed/graph_train.csv...\n",
            "Done.\n",
            ">> üóëÔ∏è Unloading model Qwen/Qwen2.5-3B-Instruct...\n",
            ">> ‚úÖ Memory cleared!\n",
            "\n",
            ">> üì¶ Zipping and downloading results...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ed32e2a2-4d20-4f35-af74-146ab8e94b38\", \"processed_data.zip\", 2645982)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}